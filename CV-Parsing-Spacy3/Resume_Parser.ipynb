{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotated data from a JSON file\n",
    "import json\n",
    "cv_data = json.load(open(r\"./data/training/train_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Harini Komaravelli Test Analyst at Oracle, Hyderabad  Hyderabad, Telangana - Email me on Indeed: indeed.com/r/Harini- Komaravelli/2659eee82e435d1b  ➢ 6 Yrs. of IT Experience in Manual and Automation testing.  WORK EXPERIENCE  QA Analyst  Oracle  Test Analyst at Oracle, Hyderabad  Infosys Ltd -  Hyderabad, Telangana -  November 2011 to February 2016  Hyderabad from Nov 2011 to Feb17 2016 ➢ Worked in Tata Consultancy Services, Hyderabad from Feb 24 to Apr 11 2017 ➢ Currently working as a Test Analyst at Oracle, Hyderabad  QA Analyst with 6 years of IT experience  Oracle  EDUCATION  MCA  Osmania University  B.Sc. in Computer Science  Osmania University  SKILLS  Functional Testing, Blue Prism, Qtp  ADDITIONAL INFORMATION  Area of Expertise:  ➢ Familiar with Agile Methodologies. ➢ Having knowledge in Energy (Petroleum) & Health Care domains. ➢ Involved in preparation of Test Scenarios. ➢ Preparing Test Data for the test cases.  https://www.indeed.com/r/Harini-Komaravelli/2659eee82e435d1b?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Harini-Komaravelli/2659eee82e435d1b?isid=rex-download&ikw=download-top&co=IN   ➢ Experienced in development and execution of Test cases effectively. ➢ Experienced in Functional testing, GUI testing, Smoke testing, Regression testing and Integration Testing ➢ Experienced in doing Accessibility testing of an application ➢ Ability to understand user Requirements, Functional and Design specifications. ➢ Good knowledge of SDLC and STLC processes. ➢ Deciding the Severity and Priority of bugs. ➢ Experience in using Microsoft Test Manager & Oracle Test Manager as Test Management Tools. ➢ Having good experience in testing windows based & web based applications. ➢ Involved in Client Interactions for reviews, issues and for any clarifications. ➢ Web Services Testing ➢ Writing Test Scripts in QTP, Testcomplete. ➢ Creating Object Repositories and Function Libraries in QTP. ➢ Enhanced QTP scripts using VB Script. ➢ Strong experience in working with Blue Prism tool ➢ Worked on different Environments like Windows Application & Web Application  Technical Skills:  ❑ Test Automation Tools: Blue Prism, QTP 10.0, Testcomplete ❑ Test Management Tool: Microsoft Test Manager, Oracle Test Manager & JIRA ❑ Databases: Oracle 10g, SQL Server.  ❑ Operating Systems: Windows 7  Project 1: Title: Cadence Client: Baker Hughes  Technologies: Microsoft Visual Studio and Microsoft Team Foundation Server  Client Background: An oilfield services company delivering focused efforts on shale gas and other oilfield services. It provides services, tools and software for drilling and formation evaluation, well completion, production management, seismic data collection and interpretation.  Project Description: AUT (Application under test) is the next generation revolutionary, robust, easy to use scalable well site data acquisition processing and interpretation system for Client's Drilling Services to deliver services that meets cross divisional business requirements consistently.  Project 2:  Description: Paragon supports your entire care team with one tool that your clinicians need to help deliver the best patient care. Designed by physicians, nurses, pharmacists and mid level providers that have a first-hand understanding of clinical workflow needs, Paragon clinical applications allow your caregivers to focus on what matters most; spending time caring for patients. Since Paragon is fully-integrated across all applications and built around a single patient database, information    entered anywhere in the system is immediately available to the entire care team. Immediate access not only helps clinicians make better treatment decisions - it also helps promote patient safety. Paragon offers a broad suite of multidisciplinary clinical software solutions together with anytime, anywhere access to the complete patient record.  Responsibilities:  • Performed Smoke testing and Regression testing. • Involved in Generating and Executing Test Script using Quick Test Pro & Blue Prism • Usability and User Interface Testing. • Involved in Defect tracking and reporting the bugs using TFS • Participated in frequent walk-through meetings with Internal Quality Assurance groups and with development groups. • Participated in client calls and clarifying the doubts by having AT&T sessions • Involved in functional, regression and smoke testing to validate the application data changes done in windows application • Certifying the build status by running the scripts as part of smoke testing  Project 3:  Description: Food & Beverages R&A: Easily manage business across multiple locations while reducing IT cost and complexity. Cloud-based point-of-sale (POS) solutions enable centralized enterprise management with lower upfront costs and a smaller footprint.  Responsibilities:  • Performed Functional testing and Regression testing. • Involved in Generating and Executing Test Scripts using Blue Prism tool and Open script • Involved in preparing bots using Blue Prism tool. • Accessibility testing of the web application • Involved in Defect tracking and reporting the bugs using JIRA • WebServices testing by calling API's to export the data\",\n",
       " {'entities': [[2275, 2281, 'Companies worked at'],\n",
       "   [2235, 2241, 'Companies worked at'],\n",
       "   [1603, 1609, 'Companies worked at'],\n",
       "   [667, 702, 'Skills'],\n",
       "   [639, 657, 'College Name'],\n",
       "   [612, 637, 'Degree'],\n",
       "   [592, 610, 'College Name'],\n",
       "   [587, 590, 'Degree'],\n",
       "   [568, 574, 'Companies worked at'],\n",
       "   [526, 536, 'Designation'],\n",
       "   [515, 524, 'Location'],\n",
       "   [507, 513, 'Companies worked at'],\n",
       "   [491, 503, 'Designation'],\n",
       "   [429, 438, 'Location'],\n",
       "   [352, 361, 'Location'],\n",
       "   [296, 305, 'Location'],\n",
       "   [270, 279, 'Location'],\n",
       "   [262, 268, 'Companies worked at'],\n",
       "   [246, 258, 'Designation'],\n",
       "   [238, 244, 'Companies worked at'],\n",
       "   [226, 236, 'Designation'],\n",
       "   [177, 207, 'Designation'],\n",
       "   [150, 155, 'Years of Experience'],\n",
       "   [54, 63, 'Location'],\n",
       "   [43, 52, 'Location'],\n",
       "   [35, 41, 'Companies worked at'],\n",
       "   [19, 31, 'Designation'],\n",
       "   [0, 18, 'Name']]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "\\data\\training\\config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config data/training/base_config.cfg /data/training/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create spaCy DocBin objects from the annotated data\n",
    "def get_spacy_doc(file, data):\n",
    "  # Create a blank spaCy pipeline\n",
    "  nlp = spacy.blank('en')\n",
    "  db = DocBin()\n",
    "\n",
    "  # Iterate through the data\n",
    "  for text, annot in tqdm(data):\n",
    "    doc = nlp.make_doc(text)\n",
    "    annot = annot['entities']\n",
    "\n",
    "    ents = []\n",
    "    entity_indices = []\n",
    "\n",
    "    # Extract entities from the annotations\n",
    "    for start, end, label in annot:\n",
    "      skip_entity = False\n",
    "      for idx in range(start, end):\n",
    "        if idx in entity_indices:\n",
    "          skip_entity = True   # if entities are overlapping\n",
    "          break\n",
    "      if skip_entity:\n",
    "        continue\n",
    "\n",
    "      entity_indices = entity_indices + list(range(start, end))\n",
    "\n",
    "      try:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode='strict')\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "      if span is None:\n",
    "        # Log errors for annotations that couldn't be processed\n",
    "        err_data = str([start, end]) + \"    \" + str(text) + \"\\n\"\n",
    "        file.write(err_data)\n",
    "      else:\n",
    "        ents.append(span)\n",
    "\n",
    "    try:\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "  return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the annotated data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(cv_data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the number of items in the training and testing sets\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:02<00:00, 64.20it/s]\n",
      "100%|██████████| 60/60 [00:01<00:00, 57.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open a file to log errors during annotation processing\n",
    "file = open('./data/model_data/error_file.txt','w', encoding='utf-8')\n",
    "\n",
    "# Create spaCy DocBin objects for training and testing data\n",
    "db = get_spacy_doc(file, train)\n",
    "db.to_disk('./data/model_data/train_data.spacy')\n",
    "\n",
    "db = get_spacy_doc(file, test)\n",
    "db.to_disk('./data/model_data/test_data.spacy')\n",
    "\n",
    "# Close the error log file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cupy\n",
      "  Using cached cupy-12.2.0.tar.gz (2.0 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [67 lines of output]\n",
      "      Clearing directory: C:\\Users\\domin\\AppData\\Local\\Temp\\pip-install-w0y5zh7a\\cupy_e64f83818c1744389c1f4f4c033cd689\\cupy\\.data\n",
      "      Looking for NVTX: C:\\Program Files\\NVIDIA Corporation\\Nsight Systems *\\target-windows-x64\\nvtx\n",
      "      NVTX could not be found\n",
      "      \n",
      "      -------- Configuring Module: cuda --------\n",
      "      Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      **************************************************\n",
      "      *** WARNING: Cannot check compute capability\n",
      "      Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      **************************************************\n",
      "      \n",
      "      ************************************************************\n",
      "      * CuPy Configuration Summary                               *\n",
      "      ************************************************************\n",
      "      \n",
      "      Build Environment:\n",
      "        Include directories: ['C:\\\\Users\\\\domin\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w0y5zh7a\\\\cupy_e64f83818c1744389c1f4f4c033cd689\\\\cupy/_core/include\\\\cupy\\\\cub', 'C:\\\\Users\\\\domin\\\\AppData\\\\Local\\\\Temp\\\\pip-install-w0y5zh7a\\\\cupy_e64f83818c1744389c1f4f4c033cd689\\\\cupy/_core/include']\n",
      "        Library directories: []\n",
      "        nvcc command       : (not found)\n",
      "        hipcc command      : (not found)\n",
      "      \n",
      "      Environment Variables:\n",
      "        CFLAGS          : (none)\n",
      "        LDFLAGS         : (none)\n",
      "        LIBRARY_PATH    : (none)\n",
      "        CUDA_PATH       : (none)\n",
      "        NVCC            : (none)\n",
      "        HIPCC           : (none)\n",
      "        ROCM_HOME       : (none)\n",
      "      \n",
      "      Modules:\n",
      "        cuda      : No\n",
      "          -> Include files not found: ['cublas_v2.h', 'cuda.h', 'cuda_profiler_api.h', 'cuda_runtime.h', 'cufft.h', 'curand.h', 'cusparse.h', 'nvrtc.h']\n",
      "          -> Check your CFLAGS environment variable.\n",
      "      \n",
      "      ERROR: CUDA could not be found on your system.\n",
      "      \n",
      "      HINT: You are trying to build CuPy from source, which is NOT recommended for general use.\n",
      "            Please consider using binary packages instead.\n",
      "      \n",
      "      Please refer to the Installation Guide for details:\n",
      "      https://docs.cupy.dev/en/stable/install.html\n",
      "      \n",
      "      ************************************************************\n",
      "      \n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 149, in prepare_metadata_for_build_wheel\n",
      "          return hook(metadata_directory, config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\domin\\AppData\\Local\\Temp\\pip-build-env-o1dwt62d\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 396, in prepare_metadata_for_build_wheel\n",
      "          self.run_setup()\n",
      "        File \"C:\\Users\\domin\\AppData\\Local\\Temp\\pip-build-env-o1dwt62d\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 507, in run_setup\n",
      "          super(_BuildMetaLegacyBackend, self).run_setup(setup_script=setup_script)\n",
      "        File \"C:\\Users\\domin\\AppData\\Local\\Temp\\pip-build-env-o1dwt62d\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 341, in run_setup\n",
      "          exec(code, locals())\n",
      "        File \"<string>\", line 88, in <module>\n",
      "        File \"C:\\Users\\domin\\AppData\\Local\\Temp\\pip-install-w0y5zh7a\\cupy_e64f83818c1744389c1f4f4c033cd689\\install\\cupy_builder\\cupy_setup_build.py\", line 449, in get_ext_modules\n",
      "          extensions = make_extensions(ctx, compiler, use_cython)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\domin\\AppData\\Local\\Temp\\pip-install-w0y5zh7a\\cupy_e64f83818c1744389c1f4f4c033cd689\\install\\cupy_builder\\cupy_setup_build.py\", line 305, in make_extensions\n",
      "          raise Exception('Your CUDA environment is invalid. '\n",
      "      Exception: Your CUDA environment is invalid. Please check above error log.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "%pip install cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "%pip install cupy==<desired_version>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: data\\model_output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\_util.py\", line 92, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\core.py\", line 778, in main\n",
      "    return _main(\n",
      "           ^^^^^^\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\click\\core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\train.py\", line 54, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\train.py\", line 76, in train\n",
      "    setup_gpu(use_gpu)\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\cli\\_util.py\", line 599, in setup_gpu\n",
      "    require_gpu(use_gpu)\n",
      "  File \"c:\\Users\\domin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\util.py\", line 230, in require_gpu\n",
      "    raise ValueError(\"Cannot use GPU, CuPy is not installed\")\n",
      "ValueError: Cannot use GPU, CuPy is not installed\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train data/training/config.cfg --output data/model_output --paths.train data/model_data/train_data.spacy --paths.dev data/model_data/test_data.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "48ec225062663513ae22d91f256f8f7625297c55a4f524abe8c1235bea13b03b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
